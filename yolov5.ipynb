{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div align=\"center\">\n\n  <a href=\"https://ultralytics.com/yolov5\" target=\"_blank\">\n    <img width=\"1024\", src=\"https://raw.githubusercontent.com/ultralytics/assets/master/yolov5/v70/splash.png\"></a>\n\n\n<br>\n  <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a>\n  <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>\n  <a href=\"https://www.kaggle.com/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n<br>\n\nThis <a href=\"https://github.com/ultralytics/yolov5\">YOLOv5</a> üöÄ notebook by <a href=\"https://ultralytics.com\">Ultralytics</a> presents simple train, validate and predict examples to help start your AI adventure.<br>See <a href=\"https://github.com/ultralytics/yolov5/issues/new/choose\">GitHub</a> for community support or <a href=\"https://ultralytics.com/contact\">contact us</a> for professional support.\n\n</div>","metadata":{"id":"t6MPjfT5NrKQ"}},{"cell_type":"markdown","source":"# Setup\n\nClone GitHub [repository](https://github.com/ultralytics/yolov5), install [dependencies](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) and check PyTorch and GPU.","metadata":{"id":"7mGmQbAO5pQb"}},{"cell_type":"code","source":"!git clone https://github.com/ultralytics/yolov5\n%cd yolov5\n%pip install -qr requirements.txt \n\nimport torch\nimport utils\ndisplay = utils.notebook_init()  ","metadata":{"id":"wbvMlHd_QwMG","outputId":"f9f016ad-3dcf-4bd2-e1c3-d5b79efc6f32","execution":{"iopub.status.busy":"2023-10-14T11:39:37.886430Z","iopub.execute_input":"2023-10-14T11:39:37.886836Z","iopub.status.idle":"2023-10-14T11:39:57.626340Z","shell.execute_reply.started":"2023-10-14T11:39:37.886804Z","shell.execute_reply":"2023-10-14T11:39:57.625403Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"YOLOv5 üöÄ v7.0-226-gdd9e338 Python-3.10.12 torch-2.0.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n","output_type":"stream"},{"name":"stdout","text":"Setup complete ‚úÖ (2 CPUs, 15.6 GB RAM, 5016.7/8062.4 GB disk)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 1. Detect\n\n`detect.py` runs YOLOv5 inference on a variety of sources, downloading models automatically from the [latest YOLOv5 release](https://github.com/ultralytics/yolov5/releases), and saving results to `runs/detect`. Example inference sources are:\n\n```shell\npython detect.py --source 0  # webcam\n                          img.jpg  # image \n                          vid.mp4  # video\n                          screen  # screenshot\n                          path/  # directory\n                         'path/*.jpg'  # glob\n                         'https://youtu.be/Zgi9g1ksQHc'  # YouTube\n                         'rtsp://example.com/media.mp4'  # RTSP, RTMP, HTTP stream\n```","metadata":{"id":"4JnkELT0cIJg"}},{"cell_type":"code","source":"!python detect.py --weights yolov5s.pt --img 640 --conf 0.25 --source /kaggle/input/dghnmm/Yolo_data (1)/Yolo_data/train/Images --data /kaggle/input/zscgfgd/custom_data1.yaml\n# display.Image(filename='runs/detect/exp/zidane.jpg', width=600)","metadata":{"id":"zR9ZbuQCH7FX","outputId":"b4db5c49-f501-4505-cf0d-a1d35236c485","execution":{"iopub.status.busy":"2023-10-14T12:18:09.974801Z","iopub.execute_input":"2023-10-14T12:18:09.975294Z","iopub.status.idle":"2023-10-14T12:18:11.060520Z","shell.execute_reply.started":"2023-10-14T12:18:09.975242Z","shell.execute_reply":"2023-10-14T12:18:11.059329Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"/bin/bash: -c: line 1: syntax error near unexpected token `('\n/bin/bash: -c: line 1: `python detect.py --weights yolov5s.pt --img 640 --conf 0.25 --source /kaggle/input/dghnmm/Yolo_data (1)/Yolo_data/train/Images --data /kaggle/input/zscgfgd/custom_data1.yaml'\n","output_type":"stream"}]},{"cell_type":"markdown","source":"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n<img align=\"left\" src=\"https://user-images.githubusercontent.com/26833433/127574988-6a558aa1-d268-44b9-bf6b-62d4c605cc72.jpg\" width=\"600\">","metadata":{"id":"hkAzDWJ7cWTr"}},{"cell_type":"markdown","source":"# 2. Validate\nValidate a model's accuracy on the [COCO](https://cocodataset.org/#home) dataset's `val` or `test` splits. Models are downloaded automatically from the [latest YOLOv5 release](https://github.com/ultralytics/yolov5/releases). To show results by class use the `--verbose` flag.","metadata":{"id":"0eq1SMWl6Sfn"}},{"cell_type":"code","source":"# Download COCO val\ntorch.hub.download_url_to_file('https://ultralytics.com/assets/coco2017val.zip', 'tmp.zip')  # download (780M - 5000 images)\n!unzip -q tmp.zip -d ../datasets && rm tmp.zip  # unzip","metadata":{"id":"WQPtK1QYVaD_","outputId":"c7d0a0d2-abfb-44c3-d60d-f99d0e7aabad","execution":{"iopub.status.busy":"2023-10-14T11:57:59.364770Z","iopub.execute_input":"2023-10-14T11:57:59.365138Z","iopub.status.idle":"2023-10-14T11:58:19.550295Z","shell.execute_reply.started":"2023-10-14T11:57:59.365108Z","shell.execute_reply":"2023-10-14T11:58:19.549070Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 780M/780M [00:10<00:00, 75.8MB/s] \n","output_type":"stream"}]},{"cell_type":"code","source":"# Validate YOLOv5s on COCO val\n!python val.py --weights yolov5s.pt --data coco.yaml --img 640 --half","metadata":{"id":"X58w8JLpMnjH","outputId":"5fc61358-7bc5-4310-a310-9059f66c6322","execution":{"iopub.status.busy":"2023-10-14T11:58:19.552655Z","iopub.execute_input":"2023-10-14T11:58:19.553712Z","iopub.status.idle":"2023-10-14T12:00:07.751101Z","shell.execute_reply.started":"2023-10-14T11:58:19.553676Z","shell.execute_reply":"2023-10-14T12:00:07.749728Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"\u001b[34m\u001b[1mval: \u001b[0mdata=/kaggle/working/yolov5/data/coco.yaml, weights=['yolov5s.pt'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.6, max_det=300, task=val, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=True, project=runs/val, name=exp, exist_ok=False, half=True, dnn=False\nYOLOv5 üöÄ v7.0-226-gdd9e338 Python-3.10.12 torch-2.0.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n\nFusing layers... \nYOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/working/datasets/coco/val2017... 4952 images, 48 backgroun\u001b[0m\n\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /kaggle/working/datasets/coco/val2017.cache\n                 Class     Images  Instances          P          R      mAP50   \n                   all       5000      36335      0.671      0.519      0.566      0.371\nSpeed: 0.1ms pre-process, 2.8ms inference, 2.0ms NMS per image at shape (32, 3, 640, 640)\n\nEvaluating pycocotools mAP... saving runs/val/exp/yolov5s_predictions.json...\n\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['pycocotools>=2.0.6'] not found, attempting AutoUpdate...\n^C\n\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n\u001b[0m\npycocotools unable to run: No module named 'pycocotools'\nResults saved to \u001b[1mruns/val/exp\u001b[0m\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 3. Train\n\n<p align=\"\"><a href=\"https://roboflow.com/?ref=ultralytics\"><img width=\"1000\" src=\"https://github.com/ultralytics/assets/raw/master/im/integrations-loop.png\"/></a></p>\nClose the active learning loop by sampling images from your inference conditions with the `roboflow` pip package\n<br><br>\n\nTrain a YOLOv5s model on the [COCO128](https://www.kaggle.com/ultralytics/coco128) dataset with `--data coco128.yaml`, starting from pretrained `--weights yolov5s.pt`, or from randomly initialized `--weights '' --cfg yolov5s.yaml`.\n\n- **Pretrained [Models](https://github.com/ultralytics/yolov5/tree/master/models)** are downloaded\nautomatically from the [latest YOLOv5 release](https://github.com/ultralytics/yolov5/releases)\n- **[Datasets](https://github.com/ultralytics/yolov5/tree/master/data)** available for autodownload include: [COCO](https://github.com/ultralytics/yolov5/blob/master/data/coco.yaml), [COCO128](https://github.com/ultralytics/yolov5/blob/master/data/coco128.yaml), [VOC](https://github.com/ultralytics/yolov5/blob/master/data/VOC.yaml), [Argoverse](https://github.com/ultralytics/yolov5/blob/master/data/Argoverse.yaml), [VisDrone](https://github.com/ultralytics/yolov5/blob/master/data/VisDrone.yaml), [GlobalWheat](https://github.com/ultralytics/yolov5/blob/master/data/GlobalWheat2020.yaml), [xView](https://github.com/ultralytics/yolov5/blob/master/data/xView.yaml), [Objects365](https://github.com/ultralytics/yolov5/blob/master/data/Objects365.yaml), [SKU-110K](https://github.com/ultralytics/yolov5/blob/master/data/SKU-110K.yaml).\n- **Training Results** are saved to `runs/train/` with incrementing run directories, i.e. `runs/train/exp2`, `runs/train/exp3` etc.\n<br><br>\n\nA **Mosaic Dataloader** is used for training which combines 4 images into 1 mosaic.\n\n## Train on Custom Data with Roboflow üåü NEW\n\n[Roboflow](https://roboflow.com/?ref=ultralytics) enables you to easily **organize, label, and prepare** a high quality dataset with your own custom data. Roboflow also makes it easy to establish an active learning pipeline, collaborate with your team on dataset improvement, and integrate directly into your model building workflow with the `roboflow` pip package.\n\n- Custom Training Example: [https://blog.roboflow.com/how-to-train-yolov5-on-a-custom-dataset/](https://blog.roboflow.com/how-to-train-yolov5-on-a-custom-dataset/?ref=ultralytics)\n- Custom Training Notebook: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/yolov5-custom-training-tutorial/blob/main/yolov5-custom-training.ipynb)\n<br>\n\n<p align=\"\"><a href=\"https://roboflow.com/?ref=ultralytics\"><img width=\"480\" src=\"https://uploads-ssl.webflow.com/5f6bc60e665f54545a1e52a5/6152a275ad4b4ac20cd2e21a_roboflow-annotate.gif\"/></a></p>Label images lightning fast (including with model-assisted labeling)","metadata":{"id":"ZY2VXXXu74w5"}},{"cell_type":"code","source":"#@title Select YOLOv5 üöÄ logger {run: 'auto'}\nlogger = 'TensorBoard' #@param ['TensorBoard', 'Comet', 'ClearML']\n\nif logger == 'TensorBoard':\n  %load_ext tensorboard\n  %tensorboard --logdir runs/train\nelif logger == 'Comet':\n  %pip install -q comet_ml\n  import comet_ml; comet_ml.init()\nelif logger == 'ClearML':\n  %pip install -q clearml\n  import clearml; clearml.browser_login()","metadata":{"id":"i3oKtE4g-aNn","execution":{"iopub.status.busy":"2023-10-14T12:09:23.116849Z","iopub.execute_input":"2023-10-14T12:09:23.117284Z","iopub.status.idle":"2023-10-14T12:09:23.133540Z","shell.execute_reply.started":"2023-10-14T12:09:23.117232Z","shell.execute_reply":"2023-10-14T12:09:23.132542Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"The tensorboard extension is already loaded. To reload it, use:\n  %reload_ext tensorboard\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Reusing TensorBoard on port 6006 (pid 293), started 0:09:11 ago. (Use '!kill 293' to kill it.)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n      <iframe id=\"tensorboard-frame-50c79585ddca25e\" width=\"100%\" height=\"800\" frameborder=\"0\">\n      </iframe>\n      <script>\n        (function() {\n          const frame = document.getElementById(\"tensorboard-frame-50c79585ddca25e\");\n          const url = new URL(\"/\", window.location);\n          const port = 6006;\n          if (port) {\n            url.port = port;\n          }\n          frame.src = url;\n        })();\n      </script>\n    "},"metadata":{}}]},{"cell_type":"code","source":"# Train YOLOv5s on COCO128 for 3 epochs\n!python train.py --img 1080 --batch 16 --epochs 3 --data /kaggle/input/zscgfgd/custom_data1.yaml --weights yolov5s.pt --cache","metadata":{"id":"1NcFxRcFdJ_O","outputId":"721b9028-767f-4a05-c964-692c245f7398","execution":{"iopub.status.busy":"2023-10-14T12:09:25.426682Z","iopub.execute_input":"2023-10-14T12:09:25.427024Z","iopub.status.idle":"2023-10-14T12:09:41.773649Z","shell.execute_reply.started":"2023-10-14T12:09:25.426989Z","shell.execute_reply":"2023-10-14T12:09:41.772516Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=/kaggle/input/zscgfgd/custom_data1.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=3, batch_size=16, imgsz=1080, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ‚úÖ\nYOLOv5 üöÄ v7.0-226-gdd9e338 Python-3.10.12 torch-2.0.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n\n\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 üöÄ runs in Comet\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\nOverriding model.yaml nc=80 with nc=2\n\n                 from  n    params  module                                  arguments                     \n  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n 24      [17, 20, 23]  1     18879  models.yolo.Detect                      [2, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\nModel summary: 214 layers, 7025023 parameters, 7025023 gradients, 16.0 GFLOPs\n\nTransferred 343/349 items from yolov5s.pt\n\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\nWARNING ‚ö†Ô∏è --img-size 1080 must be multiple of max stride 32, updating to 1088\n\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/dcvbnm/Yolo_data/train/Images... 0 images, 638 bac\u001b[0m\n\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è No labels found in /kaggle/input/dcvbnm/Yolo_data/train/Images.cache. See https://docs.ultralytics.com/yolov5/tutorials/train_custom_data\n\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è Cache directory /kaggle/input/dcvbnm/Yolo_data/train is not writeable: [Errno 30] Read-only file system: '/kaggle/input/dcvbnm/Yolo_data/train/Images.cache.npy'\nTraceback (most recent call last):\n  File \"/kaggle/working/yolov5/train.py\", line 647, in <module>\n    main(opt)\n  File \"/kaggle/working/yolov5/train.py\", line 536, in main\n    train(opt.hyp, opt, device, callbacks)\n  File \"/kaggle/working/yolov5/train.py\", line 195, in train\n    train_loader, dataset = create_dataloader(train_path,\n  File \"/kaggle/working/yolov5/utils/dataloaders.py\", line 124, in create_dataloader\n    dataset = LoadImagesAndLabels(\n  File \"/kaggle/working/yolov5/utils/dataloaders.py\", line 502, in __init__\n    assert nf > 0 or not augment, f'{prefix}No labels found in {cache_path}, can not start training. {HELP_URL}'\nAssertionError: \u001b[34m\u001b[1mtrain: \u001b[0mNo labels found in /kaggle/input/dcvbnm/Yolo_data/train/Images.cache, can not start training. See https://docs.ultralytics.com/yolov5/tutorials/train_custom_data\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 4. Visualize","metadata":{"id":"15glLzbQx5u0"}},{"cell_type":"markdown","source":"## Comet Logging and Visualization üåü NEW\n[Comet](https://bit.ly/yolov5-readme-comet) is now fully integrated with YOLOv5. Track and visualize model metrics in real time, save your hyperparameters, datasets, and model checkpoints, and visualize your model predictions with [Comet Custom Panels](https://bit.ly/yolov5-colab-comet-panels)! Comet makes sure you never lose track of your work and makes it easy to share results and collaborate across teams of all sizes! \n\nGetting started is easy:\n```shell\npip install comet_ml  # 1. install\nexport COMET_API_KEY=<Your API Key>  # 2. paste API key\npython train.py --img 640 --epochs 3 --data coco128.yaml --weights yolov5s.pt  # 3. train\n```\n\nTo learn more about all of the supported Comet features for this integration, check out the [Comet Tutorial](https://github.com/ultralytics/yolov5/tree/master/utils/loggers/comet). If you'd like to learn more about Comet, head over to our [documentation](https://bit.ly/yolov5-colab-comet-docs). Get started by trying out the Comet Colab Notebook:\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1RG0WOQyxlDlo5Km8GogJpIEJlg_5lyYO?usp=sharing)\n\n<img width=\"1920\" alt=\"yolo-ui\" src=\"https://user-images.githubusercontent.com/26833433/202851203-164e94e1-2238-46dd-91f8-de020e9d6b41.png\">","metadata":{"id":"nWOsI5wJR1o3"}},{"cell_type":"markdown","source":"## ClearML Logging and Automation üåü NEW\n\n[ClearML](https://cutt.ly/yolov5-notebook-clearml) is completely integrated into YOLOv5 to track your experimentation, manage dataset versions and even remotely execute training runs. To enable ClearML (check cells above):\n\n- `pip install clearml`\n- run `clearml-init` to connect to a ClearML server (**deploy your own [open-source server](https://github.com/allegroai/clearml-server)**, or use our [free hosted server](https://cutt.ly/yolov5-notebook-clearml))\n\nYou'll get all the great expected features from an experiment manager: live updates, model upload, experiment comparison etc. but ClearML also tracks uncommitted changes and installed packages for example. Thanks to that ClearML Tasks (which is what we call experiments) are also reproducible on different machines! With only 1 extra line, we can schedule a YOLOv5 training task on a queue to be executed by any number of ClearML Agents (workers).\n\nYou can use ClearML Data to version your dataset and then pass it to YOLOv5 simply using its unique ID. This will help you keep track of your data without adding extra hassle. Explore the [ClearML Tutorial](https://github.com/ultralytics/yolov5/tree/master/utils/loggers/clearml) for details!\n\n<a href=\"https://cutt.ly/yolov5-notebook-clearml\">\n<img alt=\"ClearML Experiment Management UI\" src=\"https://github.com/thepycoder/clearml_screenshots/raw/main/scalars.jpg\" width=\"1280\"/></a>","metadata":{"id":"Lay2WsTjNJzP"}},{"cell_type":"markdown","source":"## Local Logging\n\nTraining results are automatically logged with [Tensorboard](https://www.tensorflow.org/tensorboard) and [CSV](https://github.com/ultralytics/yolov5/pull/4148) loggers to `runs/train`, with a new experiment directory created for each new training as `runs/train/exp2`, `runs/train/exp3`, etc.\n\nThis directory contains train and val statistics, mosaics, labels, predictions and augmentated mosaics, as well as metrics and charts including precision-recall (PR) curves and confusion matrices. \n\n<img alt=\"Local logging results\" src=\"https://user-images.githubusercontent.com/26833433/183222430-e1abd1b7-782c-4cde-b04d-ad52926bf818.jpg\" width=\"1280\"/>\n","metadata":{"id":"-WPvRbS5Swl6"}},{"cell_type":"markdown","source":"# Environments\n\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\n\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://github.com/ultralytics/yolov5/wiki/GCP-Quickstart)\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://github.com/ultralytics/yolov5/wiki/AWS-Quickstart)\n- **Docker Image**. See [Docker Quickstart Guide](https://github.com/ultralytics/yolov5/wiki/Docker-Quickstart) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\n","metadata":{"id":"Zelyeqbyt3GD"}},{"cell_type":"markdown","source":"# Status\n\n![YOLOv5 CI](https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg)\n\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 training ([train.py](https://github.com/ultralytics/yolov5/blob/master/train.py)), testing ([val.py](https://github.com/ultralytics/yolov5/blob/master/val.py)), inference ([detect.py](https://github.com/ultralytics/yolov5/blob/master/detect.py)) and export ([export.py](https://github.com/ultralytics/yolov5/blob/master/export.py)) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\n","metadata":{"id":"6Qu7Iesl0p54"}},{"cell_type":"markdown","source":"# Appendix\n\nAdditional content below.","metadata":{"id":"IEijrePND_2I"}},{"cell_type":"code","source":"# YOLOv5 PyTorch HUB Inference (DetectionModels only)\nimport torch\n\nmodel = torch.hub.load('ultralytics/yolov5', 'yolov5s')  # yolov5n - yolov5x6 or custom\nim = 'https://ultralytics.com/images/zidane.jpg'  # file, Path, PIL.Image, OpenCV, nparray, list\nresults = model(im)  # inference\nresults.print()  # or .show(), .save(), .crop(), .pandas(), etc.","metadata":{"id":"GMusP4OAxFu6"},"execution_count":null,"outputs":[]}]}